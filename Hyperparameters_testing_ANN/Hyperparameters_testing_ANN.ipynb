{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f50b7c",
   "metadata": {},
   "source": [
    "# Hyperparameter testing for ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157ddbd",
   "metadata": {},
   "source": [
    "We start by importing the necessary libraries and setting the random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45340d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.callbacks import EarlyStopping, Callback\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1744de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 78 # random seed, used for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab28de",
   "metadata": {},
   "source": [
    "We now download the dataset and prepare it before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7dc67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # random crop for augmentation\n",
    "    transforms.RandomHorizontalFlip(),                  # horizontal flip for augmentation\n",
    "    transforms.ToTensor(),                              # convert to tensor\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),      # normalize by mean and std per channel\n",
    "                         (0.2470, 0.2435, 0.2616))      #These values come from \"https://github.com/kuangliu/pytorch-cifar/issues/19\" where the values for normalization were computed\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2470, 0.2435, 0.2616))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2977c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='../.data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='../.data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b3d29",
   "metadata": {},
   "source": [
    "With both datasets separated, we can now create dataloaders for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abf13490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def get_loaders(train_dataset, test_dataset, batch_size):\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4971feee",
   "metadata": {},
   "source": [
    "Finally, we will use these loaders and datasets to train our ANN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2846389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitANN(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "        input_size=32*32*3,\n",
    "        hidden_sizes=[128, 64],\n",
    "        num_classes=10,\n",
    "        activation_fn=F.relu,\n",
    "        learning_rate=0.001,\n",
    "        optimizer_name='sgd',\n",
    "        momentum=0.9,\n",
    "        dropout_rate=0.0,\n",
    "        use_batch_norm=False,\n",
    "        weight_init=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_fn = activation_fn\n",
    "        self.optimizer_name = optimizer_name.lower()\n",
    "        self.momentum = momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            \n",
    "            # Optional: Batch Normalization\n",
    "            if self.use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            \n",
    "            # Activation will be applied in forward()\n",
    "            \n",
    "            # Optional: Dropout\n",
    "            if self.dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(self.dropout_rate))\n",
    "            \n",
    "            in_features = hidden_size\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList(layers)\n",
    "        self.output_layer = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        # Apply weight initialization\n",
    "        self._initialize_weights(weight_init)\n",
    "    \n",
    "    def _initialize_weights(self, method):\n",
    "        \"\"\"Initialize weights using specified strategy\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if method == 'xavier':\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                elif method == 'he':\n",
    "                    nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                elif method == 'normal':\n",
    "                    nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "                \n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = layer(x)\n",
    "                x = self.activation_fn(x)\n",
    "            else:\n",
    "                # BatchNorm or Dropout\n",
    "                x = layer(x)\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        self.log('train_acc', acc, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_acc', acc, on_epoch=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log('test_loss', loss, on_epoch=True)\n",
    "        self.log('test_acc', acc, on_epoch=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizer based on optimizer_name\"\"\"\n",
    "        if self.optimizer_name == 'sgd':\n",
    "            return torch.optim.SGD(\n",
    "                self.parameters(), \n",
    "                lr=self.learning_rate, \n",
    "                momentum=self.momentum\n",
    "            )\n",
    "        elif self.optimizer_name == 'adam':\n",
    "            return torch.optim.Adam(\n",
    "                self.parameters(), \n",
    "                lr=self.learning_rate\n",
    "            )\n",
    "        elif self.optimizer_name == 'rmsprop':\n",
    "            return torch.optim.RMSprop(\n",
    "                self.parameters(), \n",
    "                lr=self.learning_rate\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {self.optimizer_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe405e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | hidden_layers | ModuleList | 401 K  | train\n",
      "1 | output_layer  | Linear     | 650    | train\n",
      "-----------------------------------------------------\n",
      "402 K     Trainable params\n",
      "0         Non-trainable params\n",
      "402 K     Total params\n",
      "1.609     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afons\\Desktop\\FAP\\FAP\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 57.20it/s]         | Val Loss:   2.3014 | Val Acc:   0.1172\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afons\\Desktop\\FAP\\FAP\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:15<00:00, 45.00it/s, v_num=0]         | Val Loss:   1.8773 | Val Acc:   0.3316\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:31<00:00, 22.31it/s, v_num=0]Epoch   0 | Train Loss: 2.0232 | Train Acc: 0.2712\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:31<00:00, 22.28it/s, v_num=0]         | Val Loss:   1.7688 | Val Acc:   0.3644\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:49<00:00, 14.26it/s, v_num=0]Epoch   1 | Train Loss: 1.8207 | Train Acc: 0.3490\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 29.61it/s, v_num=0]         | Val Loss:   1.7168 | Val Acc:   0.3906\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 20.02it/s, v_num=0]Epoch   2 | Train Loss: 1.7360 | Train Acc: 0.3780\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:20<00:00, 34.36it/s, v_num=0]         | Val Loss:   1.6751 | Val Acc:   0.4080\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:31<00:00, 22.47it/s, v_num=0]Epoch   3 | Train Loss: 1.6821 | Train Acc: 0.3976\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 32.58it/s, v_num=0]         | Val Loss:   1.6471 | Val Acc:   0.4114\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:32<00:00, 21.73it/s, v_num=0]Epoch   4 | Train Loss: 1.6469 | Train Acc: 0.4105\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:19<00:00, 35.53it/s, v_num=0]         | Val Loss:   1.6266 | Val Acc:   0.4212\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:30<00:00, 23.33it/s, v_num=0]Epoch   5 | Train Loss: 1.6142 | Train Acc: 0.4243\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.42it/s, v_num=0]         | Val Loss:   1.5861 | Val Acc:   0.4398\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:40<00:00, 17.54it/s, v_num=0]Epoch   6 | Train Loss: 1.5924 | Train Acc: 0.4314\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:30<00:00, 23.04it/s, v_num=0]         | Val Loss:   1.5822 | Val Acc:   0.4364\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:44<00:00, 15.85it/s, v_num=0]Epoch   7 | Train Loss: 1.5680 | Train Acc: 0.4401\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 21.12it/s, v_num=0]         | Val Loss:   1.5482 | Val Acc:   0.4484\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:48<00:00, 14.53it/s, v_num=0]Epoch   8 | Train Loss: 1.5501 | Train Acc: 0.4458\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:31<00:00, 22.15it/s, v_num=0]         | Val Loss:   1.5448 | Val Acc:   0.4512\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:46<00:00, 15.25it/s, v_num=0]Epoch   9 | Train Loss: 1.5294 | Train Acc: 0.4558\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:46<00:00, 15.24it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:46<00:00, 15.23it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\afons\\Desktop\\FAP\\FAP\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 63.72it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_acc            0.4627000093460083\n",
      "        test_loss           1.5202147960662842\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.5202147960662842, 'test_acc': 0.4627000093460083}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "HIDDEN_SIZES = [ 128, 64]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48d27800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Load the CSV file\\ncsv_path = \"logs/hs[128, 64]_optsgd_lr0.001/version_0/metrics.csv\"\\ndf = pd.read_csv(csv_path)\\n\\n# Extract test metrics (they appear only once at the end)\\ntest_row = df[df[\\'test_loss\\'].notna()]\\nif not test_row.empty:\\n    test_loss = test_row[\\'test_loss\\'].values[0]\\n    test_acc = test_row[\\'test_acc\\'].values[0]\\n\\n# Create the plots\\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\\n# Loss plot\\naxes[0].plot(df[\\'epoch\\'], df[\\'train_loss_epoch\\'], label=\\'Train Loss\\', marker=\\'o\\')\\naxes[0].plot(df[\\'epoch\\'], df[\\'val_loss\\'], label=\\'Val Loss\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[0].axhline(y=test_loss, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Loss\\')\\naxes[0].set_xlabel(\\'Epoch\\')\\naxes[0].set_ylabel(\\'Loss\\')\\naxes[0].set_title(\\'Loss Over Epochs\\')\\naxes[0].legend()\\naxes[0].grid(True)\\n\\n# Accuracy plot\\naxes[1].plot(df[\\'epoch\\'], df[\\'train_acc_epoch\\'], label=\\'Train Acc\\', marker=\\'o\\')\\naxes[1].plot(df[\\'epoch\\'], df[\\'val_acc\\'], label=\\'Val Acc\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[1].axhline(y=test_acc, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Acc\\')\\naxes[1].set_xlabel(\\'Epoch\\')\\naxes[1].set_ylabel(\\'Accuracy\\')\\naxes[1].set_title(\\'Accuracy Over Epochs\\')\\naxes[1].legend()\\naxes[1].grid(True)\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Print test results\\nif not test_row.empty:\\n    print(f\"\\nFinal Test Results:\")\\n    print(f\"Test Loss: {test_loss:.4f}\")\\n    print(f\"Test Accuracy: {test_acc:.4f}\")\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107838a",
   "metadata": {},
   "source": [
    "# Hyperparameters:\n",
    "* Number of layers and units \n",
    "* Activation functions (ReLU, Sigmoid, Tanh, others)\n",
    "* Optimizers (SGD, Adam, RMSprop) \n",
    "* Learning rate  - A\n",
    "* Batch size - A\n",
    "* Number of epochs  - A\n",
    "* Weight initialization strategies (e.g., Xavier, He) \n",
    "* Dropout rate - A\n",
    "* Batch Normalization (with vs without) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25acd2c",
   "metadata": {},
   "source": [
    "\n",
    "Due to the need of a starting point, we started by using the parameters declared on the following link: https://www.chalisebibek.com.np/image-classification-with-pytorch-lightning-simple-ann. However, as we can see, the results are insatisfactory, which means that the hyperparameters need to be optimized. The current hyperparameters are not able to capture the complexity of the data. \n",
    "\n",
    "As stated before, due to the need of optimizing hyperparameters, we decided to increase the number of layers and continuing to funnel the number of neurons to try and prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b29e48ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | hidden_layers | ModuleList | 403 K  | train\n",
      "1 | output_layer  | Linear     | 330    | train\n",
      "-----------------------------------------------------\n",
      "404 K     Trainable params\n",
      "0         Non-trainable params\n",
      "404 K     Total params\n",
      "1.616     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afons\\Desktop\\FAP\\FAP\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 106.09it/s]         | Val Loss:   2.3017 | Val Acc:   0.1172\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afons\\Desktop\\FAP\\FAP\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 40.32it/s, v_num=0]         | Val Loss:   1.9874 | Val Acc:   0.2766\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:29<00:00, 24.21it/s, v_num=0]Epoch   0 | Train Loss: 2.1666 | Train Acc: 0.2071\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:20<00:00, 35.18it/s, v_num=0]         | Val Loss:   1.8616 | Val Acc:   0.3244\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:30<00:00, 22.76it/s, v_num=0]Epoch   1 | Train Loss: 1.9269 | Train Acc: 0.3021\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 32.39it/s, v_num=0]         | Val Loss:   1.7749 | Val Acc:   0.3472\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:32<00:00, 21.58it/s, v_num=0]Epoch   2 | Train Loss: 1.8196 | Train Acc: 0.3392\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:29<00:00, 23.61it/s, v_num=0]         | Val Loss:   1.7149 | Val Acc:   0.3724\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:42<00:00, 16.63it/s, v_num=0]Epoch   3 | Train Loss: 1.7447 | Train Acc: 0.3700\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.85it/s, v_num=0]         | Val Loss:   1.6779 | Val Acc:   0.3944\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 18.67it/s, v_num=0]Epoch   4 | Train Loss: 1.6955 | Train Acc: 0.3878\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.56it/s, v_num=0]         | Val Loss:   1.6427 | Val Acc:   0.4050\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.52it/s, v_num=0]Epoch   5 | Train Loss: 1.6564 | Train Acc: 0.4037\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 29.89it/s, v_num=0]         | Val Loss:   1.6287 | Val Acc:   0.4144\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 20.86it/s, v_num=0]Epoch   6 | Train Loss: 1.6259 | Train Acc: 0.4139\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.95it/s, v_num=0]         | Val Loss:   1.6005 | Val Acc:   0.4208\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.95it/s, v_num=0]Epoch   7 | Train Loss: 1.6012 | Train Acc: 0.4246\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.74it/s, v_num=0]         | Val Loss:   1.5793 | Val Acc:   0.4182\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.05it/s, v_num=0]Epoch   8 | Train Loss: 1.5771 | Train Acc: 0.4335\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.69it/s, v_num=0]         | Val Loss:   1.5711 | Val Acc:   0.4338\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.95it/s, v_num=0]Epoch   9 | Train Loss: 1.5581 | Train Acc: 0.4436\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.95it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.93it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\afons\\Desktop\\FAP\\FAP\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 117.44it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_acc            0.4494999945163727\n",
      "        test_loss            1.545510172843933\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.545510172843933, 'test_acc': 0.4494999945163727}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "        \n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d458725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Load the CSV file\\ncsv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\\ndf = pd.read_csv(csv_path)\\n\\n# Extract test metrics (they appear only once at the end)\\ntest_row = df[df[\\'test_loss\\'].notna()]\\nif not test_row.empty:\\n    test_loss = test_row[\\'test_loss\\'].values[0]\\n    test_acc = test_row[\\'test_acc\\'].values[0]\\n\\n# Create the plots\\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\\n# Loss plot\\naxes[0].plot(df[\\'epoch\\'], df[\\'train_loss_epoch\\'], label=\\'Train Loss\\', marker=\\'o\\')\\naxes[0].plot(df[\\'epoch\\'], df[\\'val_loss\\'], label=\\'Val Loss\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[0].axhline(y=test_loss, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Loss\\')\\naxes[0].set_xlabel(\\'Epoch\\')\\naxes[0].set_ylabel(\\'Loss\\')\\naxes[0].set_title(\\'Loss Over Epochs\\')\\naxes[0].legend()\\naxes[0].grid(True)\\n\\n# Accuracy plot\\naxes[1].plot(df[\\'epoch\\'], df[\\'train_acc_epoch\\'], label=\\'Train Acc\\', marker=\\'o\\')\\naxes[1].plot(df[\\'epoch\\'], df[\\'val_acc\\'], label=\\'Val Acc\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[1].axhline(y=test_acc, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Acc\\')\\naxes[1].set_xlabel(\\'Epoch\\')\\naxes[1].set_ylabel(\\'Accuracy\\')\\naxes[1].set_title(\\'Accuracy Over Epochs\\')\\naxes[1].legend()\\naxes[1].grid(True)\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Print test results\\nif not test_row.empty:\\n    print(f\"\\nFinal Test Results:\")\\n    print(f\"Test Loss: {test_loss:.4f}\")\\n    print(f\"Test Accuracy: {test_acc:.4f}\")\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03feb35b",
   "metadata": {},
   "source": [
    "With the increase of the hidden layers, we can see that the accuracy and the loss both improved ever so slightly, without showing any signs of overfitting or underfitting. However, both metrics appear to be close to flatlining. So, in hopes of trying to discard the importance of the epochs, we tried to increase the number of epochs significantly and using early stopping with a min_delta of 0.005 to prevent overfitting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcc071b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | hidden_layers | ModuleList | 403 K  | train\n",
      "1 | output_layer  | Linear     | 330    | train\n",
      "-----------------------------------------------------\n",
      "404 K     Trainable params\n",
      "0         Non-trainable params\n",
      "404 K     Total params\n",
      "1.616     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 196.67it/s]         | Val Loss:   2.3065 | Val Acc:   0.0781\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:07<00:00, 91.79it/s, v_num=1]                  | Val Loss:   1.9685 | Val Acc:   0.2782\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.02it/s, v_num=1]Epoch   0 | Train Loss: 2.1258 | Train Acc: 0.2189\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:15<00:00, 44.70it/s, v_num=1]         | Val Loss:   1.8516 | Val Acc:   0.3312\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.23it/s, v_num=1]Epoch   1 | Train Loss: 1.9164 | Train Acc: 0.3064\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:15<00:00, 44.29it/s, v_num=1]         | Val Loss:   1.7714 | Val Acc:   0.3556\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 28.09it/s, v_num=1]Epoch   2 | Train Loss: 1.8097 | Train Acc: 0.3449\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:15<00:00, 44.17it/s, v_num=1]         | Val Loss:   1.7107 | Val Acc:   0.3770\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 28.01it/s, v_num=1]Epoch   3 | Train Loss: 1.7446 | Train Acc: 0.3673\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:15<00:00, 44.77it/s, v_num=1]         | Val Loss:   1.6783 | Val Acc:   0.3868\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.27it/s, v_num=1]Epoch   4 | Train Loss: 1.6957 | Train Acc: 0.3859\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:15<00:00, 44.77it/s, v_num=1]         | Val Loss:   1.6348 | Val Acc:   0.4006\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.18it/s, v_num=1]Epoch   5 | Train Loss: 1.6560 | Train Acc: 0.4067\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:15<00:00, 44.18it/s, v_num=1]         | Val Loss:   1.6063 | Val Acc:   0.4158\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.90it/s, v_num=1]Epoch   6 | Train Loss: 1.6311 | Train Acc: 0.4116\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.71it/s, v_num=1]         | Val Loss:   1.5952 | Val Acc:   0.4292\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.98it/s, v_num=1]Epoch   7 | Train Loss: 1.5972 | Train Acc: 0.4254\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:15<00:00, 44.84it/s, v_num=1]         | Val Loss:   1.5681 | Val Acc:   0.4362\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.29it/s, v_num=1]Epoch   8 | Train Loss: 1.5743 | Train Acc: 0.4338\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:15<00:00, 44.38it/s, v_num=1]         | Val Loss:   1.5562 | Val Acc:   0.4358\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.22it/s, v_num=1]Epoch   9 | Train Loss: 1.5555 | Train Acc: 0.4402\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.44it/s, v_num=1]         | Val Loss:   1.5462 | Val Acc:   0.4406\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.43it/s, v_num=1]Epoch  10 | Train Loss: 1.5410 | Train Acc: 0.4474\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.50it/s, v_num=1]         | Val Loss:   1.5294 | Val Acc:   0.4430\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.76it/s, v_num=1]Epoch  11 | Train Loss: 1.5234 | Train Acc: 0.4542\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.08it/s, v_num=1]         | Val Loss:   1.5047 | Val Acc:   0.4564\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.68it/s, v_num=1]Epoch  12 | Train Loss: 1.5066 | Train Acc: 0.4591\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.35it/s, v_num=1]         | Val Loss:   1.4976 | Val Acc:   0.4594\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.76it/s, v_num=1]Epoch  13 | Train Loss: 1.4958 | Train Acc: 0.4602\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.39it/s, v_num=1]         | Val Loss:   1.4868 | Val Acc:   0.4626\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.84it/s, v_num=1]Epoch  14 | Train Loss: 1.4831 | Train Acc: 0.4675\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.87it/s, v_num=1]         | Val Loss:   1.4945 | Val Acc:   0.4680\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.58it/s, v_num=1]Epoch  15 | Train Loss: 1.4743 | Train Acc: 0.4689\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:18<00:00, 38.93it/s, v_num=1]         | Val Loss:   1.4901 | Val Acc:   0.4548\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:28<00:00, 24.46it/s, v_num=1]Epoch  16 | Train Loss: 1.4642 | Train Acc: 0.4725\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 39.78it/s, v_num=1]         | Val Loss:   1.4560 | Val Acc:   0.4796\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:27<00:00, 25.96it/s, v_num=1]Epoch  17 | Train Loss: 1.4552 | Train Acc: 0.4792\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.32it/s, v_num=1]         | Val Loss:   1.4563 | Val Acc:   0.4632\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.22it/s, v_num=1]Epoch  18 | Train Loss: 1.4454 | Train Acc: 0.4800\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.36it/s, v_num=1]         | Val Loss:   1.4528 | Val Acc:   0.4658\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.20it/s, v_num=1]Epoch  19 | Train Loss: 1.4389 | Train Acc: 0.4806\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.36it/s, v_num=1]         | Val Loss:   1.4568 | Val Acc:   0.4718\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.05it/s, v_num=1]Epoch  20 | Train Loss: 1.4349 | Train Acc: 0.4828\n",
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.95it/s, v_num=1]         | Val Loss:   1.4450 | Val Acc:   0.4776\n",
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.03it/s, v_num=1]Epoch  21 | Train Loss: 1.4298 | Train Acc: 0.4904\n",
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.03it/s, v_num=1]         | Val Loss:   1.4318 | Val Acc:   0.4836\n",
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.98it/s, v_num=1]Epoch  22 | Train Loss: 1.4222 | Train Acc: 0.4891\n",
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.74it/s, v_num=1]         | Val Loss:   1.4139 | Val Acc:   0.4862\n",
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.06it/s, v_num=1]Epoch  23 | Train Loss: 1.4150 | Train Acc: 0.4895\n",
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.24it/s, v_num=1]         | Val Loss:   1.4333 | Val Acc:   0.4806\n",
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.24it/s, v_num=1]Epoch  24 | Train Loss: 1.4070 | Train Acc: 0.4935\n",
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.45it/s, v_num=1]         | Val Loss:   1.4331 | Val Acc:   0.4886\n",
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.32it/s, v_num=1]Epoch  25 | Train Loss: 1.4031 | Train Acc: 0.4970\n",
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.01it/s, v_num=1]         | Val Loss:   1.4000 | Val Acc:   0.4986\n",
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.10it/s, v_num=1]Epoch  26 | Train Loss: 1.3998 | Train Acc: 0.4976\n",
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.43it/s, v_num=1]         | Val Loss:   1.4160 | Val Acc:   0.4926\n",
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.86it/s, v_num=1]Epoch  27 | Train Loss: 1.3918 | Train Acc: 0.5012\n",
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.59it/s, v_num=1]         | Val Loss:   1.4300 | Val Acc:   0.4856\n",
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.77it/s, v_num=1]Epoch  28 | Train Loss: 1.3879 | Train Acc: 0.5027\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.43it/s, v_num=1]         | Val Loss:   1.4152 | Val Acc:   0.4880\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.33it/s, v_num=1]Epoch  29 | Train Loss: 1.3898 | Train Acc: 0.5007\n",
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 40.88it/s, v_num=1]         | Val Loss:   1.4086 | Val Acc:   0.4866\n",
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.53it/s, v_num=1]Epoch  30 | Train Loss: 1.3856 | Train Acc: 0.5006\n",
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.00it/s, v_num=1]         | Val Loss:   1.3922 | Val Acc:   0.4906\n",
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.89it/s, v_num=1]Epoch  31 | Train Loss: 1.3750 | Train Acc: 0.5067\n",
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.19it/s, v_num=1]         | Val Loss:   1.3932 | Val Acc:   0.4964\n",
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.06it/s, v_num=1]Epoch  32 | Train Loss: 1.3706 | Train Acc: 0.5069\n",
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 39.58it/s, v_num=1]         | Val Loss:   1.4002 | Val Acc:   0.4960\n",
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:27<00:00, 25.79it/s, v_num=1]Epoch  33 | Train Loss: 1.3679 | Train Acc: 0.5088\n",
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 39.68it/s, v_num=1]         | Val Loss:   1.3880 | Val Acc:   0.5044\n",
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:27<00:00, 25.99it/s, v_num=1]Epoch  34 | Train Loss: 1.3643 | Train Acc: 0.5084\n",
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 40.44it/s, v_num=1]         | Val Loss:   1.3907 | Val Acc:   0.4976\n",
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.41it/s, v_num=1]Epoch  35 | Train Loss: 1.3615 | Train Acc: 0.5127\n",
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.04it/s, v_num=1]         | Val Loss:   1.3954 | Val Acc:   0.5000\n",
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.23it/s, v_num=1]Epoch  36 | Train Loss: 1.3567 | Train Acc: 0.5129\n",
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 40.92it/s, v_num=1]         | Val Loss:   1.3787 | Val Acc:   0.5056\n",
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.37it/s, v_num=1]Epoch  37 | Train Loss: 1.3569 | Train Acc: 0.5133\n",
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.42it/s, v_num=1]         | Val Loss:   1.3753 | Val Acc:   0.5020\n",
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.12it/s, v_num=1]Epoch  38 | Train Loss: 1.3481 | Train Acc: 0.5145\n",
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.47it/s, v_num=1]         | Val Loss:   1.3649 | Val Acc:   0.5186\n",
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.29it/s, v_num=1]Epoch  39 | Train Loss: 1.3452 | Train Acc: 0.5162\n",
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 40.94it/s, v_num=1]         | Val Loss:   1.3789 | Val Acc:   0.5010\n",
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.57it/s, v_num=1]Epoch  40 | Train Loss: 1.3413 | Train Acc: 0.5174\n",
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.80it/s, v_num=1]         | Val Loss:   1.3839 | Val Acc:   0.5098\n",
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.37it/s, v_num=1]Epoch  41 | Train Loss: 1.3425 | Train Acc: 0.5181\n",
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.62it/s, v_num=1]         | Val Loss:   1.3745 | Val Acc:   0.5066\n",
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.40it/s, v_num=1]Epoch  42 | Train Loss: 1.3374 | Train Acc: 0.5199\n",
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.80it/s, v_num=1]         | Val Loss:   1.3621 | Val Acc:   0.5036\n",
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.93it/s, v_num=1]Epoch  43 | Train Loss: 1.3323 | Train Acc: 0.5236\n",
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.93it/s, v_num=1]         | Val Loss:   1.3590 | Val Acc:   0.5222\n",
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:27<00:00, 25.94it/s, v_num=1]Epoch  44 | Train Loss: 1.3297 | Train Acc: 0.5218\n",
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 33.35it/s, v_num=1]         | Val Loss:   1.3692 | Val Acc:   0.5144\n",
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 18.57it/s, v_num=1]Epoch  45 | Train Loss: 1.3263 | Train Acc: 0.5227\n",
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 21.28it/s, v_num=1]         | Val Loss:   1.3605 | Val Acc:   0.5142\n",
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:48<00:00, 14.64it/s, v_num=1]Epoch  46 | Train Loss: 1.3262 | Train Acc: 0.5218\n",
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:31<00:00, 22.47it/s, v_num=1]         | Val Loss:   1.3589 | Val Acc:   0.5094\n",
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:46<00:00, 15.09it/s, v_num=1]Epoch  47 | Train Loss: 1.3228 | Train Acc: 0.5258\n",
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:29<00:00, 23.72it/s, v_num=1]         | Val Loss:   1.3455 | Val Acc:   0.5184\n",
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:43<00:00, 16.09it/s, v_num=1]Epoch  48 | Train Loss: 1.3233 | Train Acc: 0.5230\n",
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:30<00:00, 23.16it/s, v_num=1]         | Val Loss:   1.3485 | Val Acc:   0.5156\n",
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:45<00:00, 15.32it/s, v_num=1]Epoch  49 | Train Loss: 1.3169 | Train Acc: 0.5278\n",
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:29<00:00, 23.93it/s, v_num=1]         | Val Loss:   1.3441 | Val Acc:   0.5112\n",
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:43<00:00, 16.12it/s, v_num=1]Epoch  50 | Train Loss: 1.3148 | Train Acc: 0.5271\n",
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:30<00:00, 23.09it/s, v_num=1]         | Val Loss:   1.3515 | Val Acc:   0.5128\n",
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:44<00:00, 15.75it/s, v_num=1]Epoch  51 | Train Loss: 1.3109 | Train Acc: 0.5302\n",
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:28<00:00, 24.45it/s, v_num=1]         | Val Loss:   1.3754 | Val Acc:   0.5118\n",
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:43<00:00, 16.20it/s, v_num=1]Epoch  52 | Train Loss: 1.3129 | Train Acc: 0.5289\n",
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:54<00:00, 12.97it/s, v_num=1]         | Val Loss:   1.3375 | Val Acc:   0.5232\n",
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:23<00:00,  8.42it/s, v_num=1]Epoch  53 | Train Loss: 1.3099 | Train Acc: 0.5290\n",
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:01<00:00, 11.46it/s, v_num=1]         | Val Loss:   1.3509 | Val Acc:   0.5120\n",
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:30<00:00,  7.80it/s, v_num=1]Epoch  54 | Train Loss: 1.3048 | Train Acc: 0.5302\n",
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:32<00:00,  7.64it/s, v_num=1]          | Val Loss:   1.3392 | Val Acc:   0.5164\n",
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:57<00:00,  5.97it/s, v_num=1]Epoch  55 | Train Loss: 1.3010 | Train Acc: 0.5352\n",
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:30<00:00,  7.82it/s, v_num=1]         | Val Loss:   1.3275 | Val Acc:   0.5204\n",
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:53<00:00,  6.19it/s, v_num=1]Epoch  56 | Train Loss: 1.3011 | Train Acc: 0.5330\n",
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:55<00:00, 12.59it/s, v_num=1]         | Val Loss:   1.3305 | Val Acc:   0.5244\n",
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:15<00:00,  9.34it/s, v_num=1]Epoch  57 | Train Loss: 1.2987 | Train Acc: 0.5354\n",
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:30<00:00, 22.85it/s, v_num=1]         | Val Loss:   1.3378 | Val Acc:   0.5212\n",
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:52<00:00, 13.34it/s, v_num=1]Epoch  58 | Train Loss: 1.2916 | Train Acc: 0.5358\n",
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:01<00:00, 11.44it/s, v_num=1]         | Val Loss:   1.3286 | Val Acc:   0.5178\n",
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:28<00:00,  7.98it/s, v_num=1]Epoch  59 | Train Loss: 1.2929 | Train Acc: 0.5351\n",
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:51<00:00, 13.57it/s, v_num=1]         | Val Loss:   1.3384 | Val Acc:   0.5190\n",
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:14<00:00,  9.46it/s, v_num=1]Epoch  60 | Train Loss: 1.2942 | Train Acc: 0.5358\n",
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:56<00:00, 12.39it/s, v_num=1]         | Val Loss:   1.3163 | Val Acc:   0.5310\n",
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:20<00:00,  8.72it/s, v_num=1]Epoch  61 | Train Loss: 1.2875 | Train Acc: 0.5390\n",
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:57<00:00, 12.29it/s, v_num=1]         | Val Loss:   1.3383 | Val Acc:   0.5172\n",
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:22<00:00,  8.56it/s, v_num=1]Epoch  62 | Train Loss: 1.2894 | Train Acc: 0.5352\n",
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:13<00:00,  9.62it/s, v_num=1]         | Val Loss:   1.3358 | Val Acc:   0.5230\n",
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:35<00:00,  7.36it/s, v_num=1]Epoch  63 | Train Loss: 1.2858 | Train Acc: 0.5379\n",
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 19.61it/s, v_num=1]         | Val Loss:   1.3065 | Val Acc:   0.5320\n",
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:59<00:00, 11.89it/s, v_num=1]Epoch  64 | Train Loss: 1.2829 | Train Acc: 0.5381\n",
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:56<00:00, 12.54it/s, v_num=1]         | Val Loss:   1.3209 | Val Acc:   0.5266\n",
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:21<00:00,  8.63it/s, v_num=1]Epoch  65 | Train Loss: 1.2780 | Train Acc: 0.5396\n",
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:50<00:00, 13.99it/s, v_num=1]         | Val Loss:   1.3265 | Val Acc:   0.5206\n",
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:16<00:00,  9.22it/s, v_num=1]Epoch  66 | Train Loss: 1.2796 | Train Acc: 0.5421\n",
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:48<00:00, 14.56it/s, v_num=1]         | Val Loss:   1.3175 | Val Acc:   0.5256\n",
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:11<00:00,  9.78it/s, v_num=1]Epoch  67 | Train Loss: 1.2726 | Train Acc: 0.5427\n",
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:49<00:00, 14.20it/s, v_num=1]         | Val Loss:   1.3026 | Val Acc:   0.5352\n",
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [01:10<00:00, 10.01it/s, v_num=1]Epoch  68 | Train Loss: 1.2795 | Train Acc: 0.5385\n",
      "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.96it/s, v_num=1]         | Val Loss:   1.3135 | Val Acc:   0.5194\n",
      "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:52<00:00, 13.29it/s, v_num=1]Epoch  69 | Train Loss: 1.2742 | Train Acc: 0.5447\n",
      "Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:27<00:00, 25.46it/s, v_num=1]         | Val Loss:   1.3199 | Val Acc:   0.5280\n",
      "Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:42<00:00, 16.40it/s, v_num=1]Epoch  70 | Train Loss: 1.2719 | Train Acc: 0.5417\n",
      "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:28<00:00, 25.13it/s, v_num=1]         | Val Loss:   1.3097 | Val Acc:   0.5272\n",
      "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:41<00:00, 17.07it/s, v_num=1]Epoch  71 | Train Loss: 1.2694 | Train Acc: 0.5450\n",
      "Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.57it/s, v_num=1]         | Val Loss:   1.3223 | Val Acc:   0.5206\n",
      "Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.75it/s, v_num=1]Epoch  72 | Train Loss: 1.2686 | Train Acc: 0.5442\n",
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.83it/s, v_num=1]         | Val Loss:   1.3247 | Val Acc:   0.5290\n",
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.76it/s, v_num=1]Epoch  73 | Train Loss: 1.2676 | Train Acc: 0.5468\n",
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.75it/s, v_num=1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 99.84it/s] \n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_acc            0.5300999879837036\n",
      "        test_loss           1.3160182237625122\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.3160182237625122, 'test_acc': 0.5300999879837036}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1ee4f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Load the CSV file\\ncsv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\\ndf = pd.read_csv(csv_path)\\n\\n# Extract test metrics (they appear only once at the end)\\ntest_row = df[df[\\'test_loss\\'].notna()]\\nif not test_row.empty:\\n    test_loss = test_row[\\'test_loss\\'].values[0]\\n    test_acc = test_row[\\'test_acc\\'].values[0]\\n\\n# Create the plots\\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\\n# Loss plot\\naxes[0].plot(df[\\'epoch\\'], df[\\'train_loss_epoch\\'], label=\\'Train Loss\\', marker=\\'o\\')\\naxes[0].plot(df[\\'epoch\\'], df[\\'val_loss\\'], label=\\'Val Loss\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[0].axhline(y=test_loss, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Loss\\')\\naxes[0].set_xlabel(\\'Epoch\\')\\naxes[0].set_ylabel(\\'Loss\\')\\naxes[0].set_title(\\'Loss Over Epochs\\')\\naxes[0].legend()\\naxes[0].grid(True)\\n\\n# Accuracy plot\\naxes[1].plot(df[\\'epoch\\'], df[\\'train_acc_epoch\\'], label=\\'Train Acc\\', marker=\\'o\\')\\naxes[1].plot(df[\\'epoch\\'], df[\\'val_acc\\'], label=\\'Val Acc\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[1].axhline(y=test_acc, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Acc\\')\\naxes[1].set_xlabel(\\'Epoch\\')\\naxes[1].set_ylabel(\\'Accuracy\\')\\naxes[1].set_title(\\'Accuracy Over Epochs\\')\\naxes[1].legend()\\naxes[1].grid(True)\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Print test results\\nif not test_row.empty:\\n    print(f\"\\nFinal Test Results:\")\\n    print(f\"Test Loss: {test_loss:.4f}\")\\n    print(f\"Test Accuracy: {test_acc:.4f}\")\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81722bf6",
   "metadata": {},
   "source": [
    "As we can see, there was a slight increase on performance on both the accuracy and the loss, but even though the number of epochs was set to 100, the training was stopped by the early stopping method, meaning it didn't improve enough to keep training. So, from now on, the number of epochs will always be 100 with early stopping of min_delta = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f432458",
   "metadata": {},
   "source": [
    "Since the early testing showed that increasing the number of hidden layers improved the performance, we decided to add one more hidden layer and test the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2dda1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | hidden_layers | ModuleList | 404 K  | train\n",
      "1 | output_layer  | Linear     | 170    | train\n",
      "-----------------------------------------------------\n",
      "404 K     Trainable params\n",
      "0         Non-trainable params\n",
      "404 K     Total params\n",
      "1.618     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 96.61it/s]         | Val Loss:   2.3035 | Val Acc:   0.0781\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:14<00:00, 49.87it/s, v_num=0]                 | Val Loss:   2.2848 | Val Acc:   0.1298\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.51it/s, v_num=0]Epoch   0 | Train Loss: 2.2975 | Train Acc: 0.1136\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.83it/s, v_num=0]         | Val Loss:   2.0675 | Val Acc:   0.2370\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 18.61it/s, v_num=0]Epoch   1 | Train Loss: 2.2038 | Train Acc: 0.1767\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.10it/s, v_num=0]         | Val Loss:   1.9553 | Val Acc:   0.3046\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.19it/s, v_num=0]Epoch   2 | Train Loss: 2.0148 | Train Acc: 0.2632\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.36it/s, v_num=0]         | Val Loss:   1.8530 | Val Acc:   0.3302\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.19it/s, v_num=0]Epoch   3 | Train Loss: 1.9191 | Train Acc: 0.3017\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.26it/s, v_num=0]         | Val Loss:   1.7754 | Val Acc:   0.3638\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.93it/s, v_num=0]Epoch   4 | Train Loss: 1.8314 | Train Acc: 0.3338\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.38it/s, v_num=0]         | Val Loss:   1.7281 | Val Acc:   0.3764\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.24it/s, v_num=0]Epoch   5 | Train Loss: 1.7671 | Train Acc: 0.3542\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:29<00:00, 23.73it/s, v_num=0]         | Val Loss:   1.6746 | Val Acc:   0.3978\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:43<00:00, 16.22it/s, v_num=0]Epoch   6 | Train Loss: 1.7181 | Train Acc: 0.3707\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.19it/s, v_num=0]         | Val Loss:   1.6594 | Val Acc:   0.4000\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:40<00:00, 17.21it/s, v_num=0]Epoch   7 | Train Loss: 1.6784 | Train Acc: 0.3895\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.07it/s, v_num=0]         | Val Loss:   1.6063 | Val Acc:   0.4244\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.36it/s, v_num=0]Epoch   8 | Train Loss: 1.6529 | Train Acc: 0.3990\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.40it/s, v_num=0]         | Val Loss:   1.5972 | Val Acc:   0.4216\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:36<00:00, 19.26it/s, v_num=0]Epoch   9 | Train Loss: 1.6218 | Train Acc: 0.4108\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.49it/s, v_num=0]         | Val Loss:   1.5836 | Val Acc:   0.4256\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.39it/s, v_num=0]Epoch  10 | Train Loss: 1.5974 | Train Acc: 0.4211\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:28<00:00, 24.93it/s, v_num=0]         | Val Loss:   1.5586 | Val Acc:   0.4402\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:41<00:00, 16.90it/s, v_num=0]Epoch  11 | Train Loss: 1.5719 | Train Acc: 0.4329\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.90it/s, v_num=0]         | Val Loss:   1.5395 | Val Acc:   0.4466\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 18.01it/s, v_num=0]Epoch  12 | Train Loss: 1.5548 | Train Acc: 0.4372\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.50it/s, v_num=0]         | Val Loss:   1.5313 | Val Acc:   0.4448\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 18.76it/s, v_num=0]Epoch  13 | Train Loss: 1.5397 | Train Acc: 0.4460\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.31it/s, v_num=0]         | Val Loss:   1.5090 | Val Acc:   0.4620\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 19.00it/s, v_num=0]Epoch  14 | Train Loss: 1.5196 | Train Acc: 0.4515\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.40it/s, v_num=0]         | Val Loss:   1.4905 | Val Acc:   0.4682\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 18.85it/s, v_num=0]Epoch  15 | Train Loss: 1.5123 | Train Acc: 0.4536\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 29.38it/s, v_num=0]         | Val Loss:   1.4834 | Val Acc:   0.4632\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:36<00:00, 19.43it/s, v_num=0]Epoch  16 | Train Loss: 1.4973 | Train Acc: 0.4600\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.61it/s, v_num=0]         | Val Loss:   1.4746 | Val Acc:   0.4682\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.46it/s, v_num=0]Epoch  17 | Train Loss: 1.4889 | Train Acc: 0.4622\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.89it/s, v_num=0]         | Val Loss:   1.4512 | Val Acc:   0.4784\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.42it/s, v_num=0]Epoch  18 | Train Loss: 1.4789 | Train Acc: 0.4668\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.25it/s, v_num=0]         | Val Loss:   1.4723 | Val Acc:   0.4668\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.25it/s, v_num=0]Epoch  19 | Train Loss: 1.4636 | Train Acc: 0.4719\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:27<00:00, 25.47it/s, v_num=0]         | Val Loss:   1.4558 | Val Acc:   0.4726\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:41<00:00, 17.01it/s, v_num=0]Epoch  20 | Train Loss: 1.4638 | Train Acc: 0.4709\n",
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.51it/s, v_num=0]         | Val Loss:   1.4446 | Val Acc:   0.4852\n",
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.11it/s, v_num=0]Epoch  21 | Train Loss: 1.4543 | Train Acc: 0.4754\n",
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.22it/s, v_num=0]         | Val Loss:   1.4337 | Val Acc:   0.4786\n",
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.67it/s, v_num=0]Epoch  22 | Train Loss: 1.4469 | Train Acc: 0.4794\n",
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.66it/s, v_num=0]         | Val Loss:   1.4356 | Val Acc:   0.4956\n",
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.71it/s, v_num=0]Epoch  23 | Train Loss: 1.4340 | Train Acc: 0.4838\n",
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.92it/s, v_num=0]         | Val Loss:   1.4195 | Val Acc:   0.4914\n",
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 18.92it/s, v_num=0]Epoch  24 | Train Loss: 1.4324 | Train Acc: 0.4858\n",
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.77it/s, v_num=0]         | Val Loss:   1.4201 | Val Acc:   0.4864\n",
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.52it/s, v_num=0]Epoch  25 | Train Loss: 1.4255 | Train Acc: 0.4869\n",
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.59it/s, v_num=0]         | Val Loss:   1.4367 | Val Acc:   0.4836\n",
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 18.56it/s, v_num=0]Epoch  26 | Train Loss: 1.4231 | Train Acc: 0.4863\n",
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.80it/s, v_num=0]         | Val Loss:   1.4047 | Val Acc:   0.4956\n",
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.36it/s, v_num=0]Epoch  27 | Train Loss: 1.4163 | Train Acc: 0.4916\n",
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.70it/s, v_num=0]         | Val Loss:   1.4109 | Val Acc:   0.4906\n",
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.28it/s, v_num=0]Epoch  28 | Train Loss: 1.4087 | Train Acc: 0.4949\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.02it/s, v_num=0]         | Val Loss:   1.4041 | Val Acc:   0.4954\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.08it/s, v_num=0]Epoch  29 | Train Loss: 1.4039 | Train Acc: 0.4945\n",
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.83it/s, v_num=0]         | Val Loss:   1.3874 | Val Acc:   0.5002\n",
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.09it/s, v_num=0]Epoch  30 | Train Loss: 1.3973 | Train Acc: 0.4965\n",
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:27<00:00, 25.93it/s, v_num=0]         | Val Loss:   1.3978 | Val Acc:   0.5026\n",
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:40<00:00, 17.41it/s, v_num=0]Epoch  31 | Train Loss: 1.3909 | Train Acc: 0.4984\n",
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.96it/s, v_num=0]         | Val Loss:   1.3886 | Val Acc:   0.4992\n",
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 18.61it/s, v_num=0]Epoch  32 | Train Loss: 1.3872 | Train Acc: 0.5010\n",
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:29<00:00, 24.27it/s, v_num=0]         | Val Loss:   1.4057 | Val Acc:   0.4934\n",
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:42<00:00, 16.43it/s, v_num=0]Epoch  33 | Train Loss: 1.3838 | Train Acc: 0.5030\n",
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 28.04it/s, v_num=0]         | Val Loss:   1.3916 | Val Acc:   0.4948\n",
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.63it/s, v_num=0]Epoch  34 | Train Loss: 1.3748 | Train Acc: 0.5047\n",
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:28<00:00, 25.02it/s, v_num=0]         | Val Loss:   1.3903 | Val Acc:   0.4964\n",
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:40<00:00, 17.42it/s, v_num=0]Epoch  35 | Train Loss: 1.3752 | Train Acc: 0.5036\n",
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:40<00:00, 17.40it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.98it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_acc            0.5095000267028809\n",
      "        test_loss           1.3945926427841187\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.3945926427841187, 'test_acc': 0.5095000267028809}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32, 16]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bd3a98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Load the CSV file\\ncsv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\\ndf = pd.read_csv(csv_path)\\n\\n# Extract test metrics (they appear only once at the end)\\ntest_row = df[df[\\'test_loss\\'].notna()]\\nif not test_row.empty:\\n    test_loss = test_row[\\'test_loss\\'].values[0]\\n    test_acc = test_row[\\'test_acc\\'].values[0]\\n\\n# Create the plots\\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\\n# Loss plot\\naxes[0].plot(df[\\'epoch\\'], df[\\'train_loss_epoch\\'], label=\\'Train Loss\\', marker=\\'o\\')\\naxes[0].plot(df[\\'epoch\\'], df[\\'val_loss\\'], label=\\'Val Loss\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[0].axhline(y=test_loss, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Loss\\')\\naxes[0].set_xlabel(\\'Epoch\\')\\naxes[0].set_ylabel(\\'Loss\\')\\naxes[0].set_title(\\'Loss Over Epochs\\')\\naxes[0].legend()\\naxes[0].grid(True)\\n\\n# Accuracy plot\\naxes[1].plot(df[\\'epoch\\'], df[\\'train_acc_epoch\\'], label=\\'Train Acc\\', marker=\\'o\\')\\naxes[1].plot(df[\\'epoch\\'], df[\\'val_acc\\'], label=\\'Val Acc\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[1].axhline(y=test_acc, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Acc\\')\\naxes[1].set_xlabel(\\'Epoch\\')\\naxes[1].set_ylabel(\\'Accuracy\\')\\naxes[1].set_title(\\'Accuracy Over Epochs\\')\\naxes[1].legend()\\naxes[1].grid(True)\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Print test results\\nif not test_row.empty:\\n    print(f\"\\nFinal Test Results:\")\\n    print(f\"Test Loss: {test_loss:.4f}\")\\n    print(f\"Test Accuracy: {test_acc:.4f}\")\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131544c1",
   "metadata": {},
   "source": [
    "As we can see, the increasing of the number of hidden layers (with funneling ) didn't provide any significant improve in accuracy and actually a worst value for loss. For that reason, we decided to go back to the last layout (without the last layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3bce56",
   "metadata": {},
   "source": [
    "Next, we decided to test different activation functions, to see which one is better for the task at hands. We will start with the leaky relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87180a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | hidden_layers | ModuleList | 403 K  | train\n",
      "1 | output_layer  | Linear     | 330    | train\n",
      "-----------------------------------------------------\n",
      "404 K     Trainable params\n",
      "0         Non-trainable params\n",
      "404 K     Total params\n",
      "1.616     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 29.74it/s]         | Val Loss:   2.3075 | Val Acc:   0.0859\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:12<00:00, 56.80it/s, v_num=2]                 | Val Loss:   1.9980 | Val Acc:   0.2772\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.21it/s, v_num=2]Epoch   0 | Train Loss: 2.1555 | Train Acc: 0.1946\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 30.73it/s, v_num=2]         | Val Loss:   1.8821 | Val Acc:   0.3250\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 20.08it/s, v_num=2]Epoch   1 | Train Loss: 1.9307 | Train Acc: 0.3033\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 29.05it/s, v_num=2]         | Val Loss:   1.7906 | Val Acc:   0.3496\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:36<00:00, 19.24it/s, v_num=2]Epoch   2 | Train Loss: 1.8299 | Train Acc: 0.3379\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.44it/s, v_num=2]         | Val Loss:   1.7328 | Val Acc:   0.3636\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.44it/s, v_num=2]Epoch   3 | Train Loss: 1.7545 | Train Acc: 0.3664\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 30.44it/s, v_num=2]         | Val Loss:   1.6939 | Val Acc:   0.3790\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 19.80it/s, v_num=2]Epoch   4 | Train Loss: 1.7064 | Train Acc: 0.3851\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 29.38it/s, v_num=2]         | Val Loss:   1.6643 | Val Acc:   0.3958\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:36<00:00, 19.34it/s, v_num=2]Epoch   5 | Train Loss: 1.6675 | Train Acc: 0.3985\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 29.72it/s, v_num=2]         | Val Loss:   1.6195 | Val Acc:   0.4182\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:36<00:00, 19.49it/s, v_num=2]Epoch   6 | Train Loss: 1.6369 | Train Acc: 0.4101\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 29.40it/s, v_num=2]         | Val Loss:   1.5959 | Val Acc:   0.4238\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:36<00:00, 19.15it/s, v_num=2]Epoch   7 | Train Loss: 1.6039 | Train Acc: 0.4234\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 32.36it/s, v_num=2]         | Val Loss:   1.5673 | Val Acc:   0.4310\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 20.88it/s, v_num=2]Epoch   8 | Train Loss: 1.5827 | Train Acc: 0.4348\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.45it/s, v_num=2]         | Val Loss:   1.5793 | Val Acc:   0.4218\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 20.81it/s, v_num=2]Epoch   9 | Train Loss: 1.5636 | Train Acc: 0.4416\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.56it/s, v_num=2]         | Val Loss:   1.5510 | Val Acc:   0.4366\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.38it/s, v_num=2]Epoch  10 | Train Loss: 1.5455 | Train Acc: 0.4451\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.55it/s, v_num=2]         | Val Loss:   1.5329 | Val Acc:   0.4396\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.63it/s, v_num=2]Epoch  11 | Train Loss: 1.5328 | Train Acc: 0.4476\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 32.53it/s, v_num=2]         | Val Loss:   1.5281 | Val Acc:   0.4596\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 21.29it/s, v_num=2]Epoch  12 | Train Loss: 1.5208 | Train Acc: 0.4548\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 32.72it/s, v_num=2]         | Val Loss:   1.5018 | Val Acc:   0.4578\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 21.28it/s, v_num=2]Epoch  13 | Train Loss: 1.5084 | Train Acc: 0.4570\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.06it/s, v_num=2]         | Val Loss:   1.4923 | Val Acc:   0.4628\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.39it/s, v_num=2]Epoch  14 | Train Loss: 1.5033 | Train Acc: 0.4591\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.22it/s, v_num=2]         | Val Loss:   1.5014 | Val Acc:   0.4570\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:36<00:00, 19.42it/s, v_num=2]Epoch  15 | Train Loss: 1.4856 | Train Acc: 0.4659\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 30.68it/s, v_num=2]         | Val Loss:   1.4780 | Val Acc:   0.4706\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 20.01it/s, v_num=2]Epoch  16 | Train Loss: 1.4735 | Train Acc: 0.4714\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.51it/s, v_num=2]         | Val Loss:   1.4605 | Val Acc:   0.4744\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 20.88it/s, v_num=2]Epoch  17 | Train Loss: 1.4624 | Train Acc: 0.4758\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 32.80it/s, v_num=2]         | Val Loss:   1.4554 | Val Acc:   0.4734\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:32<00:00, 21.51it/s, v_num=2]Epoch  18 | Train Loss: 1.4605 | Train Acc: 0.4766\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 32.71it/s, v_num=2]         | Val Loss:   1.4657 | Val Acc:   0.4770\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 21.25it/s, v_num=2]Epoch  19 | Train Loss: 1.4503 | Train Acc: 0.4774\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.59it/s, v_num=2]         | Val Loss:   1.4563 | Val Acc:   0.4622\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.51it/s, v_num=2]Epoch  20 | Train Loss: 1.4450 | Train Acc: 0.4817\n",
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 30.84it/s, v_num=2]         | Val Loss:   1.4531 | Val Acc:   0.4810\n",
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.27it/s, v_num=2]Epoch  21 | Train Loss: 1.4353 | Train Acc: 0.4826\n",
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 32.13it/s, v_num=2]         | Val Loss:   1.4437 | Val Acc:   0.4794\n",
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 20.80it/s, v_num=2]Epoch  22 | Train Loss: 1.4285 | Train Acc: 0.4879\n",
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.55it/s, v_num=2]         | Val Loss:   1.4283 | Val Acc:   0.4898\n",
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:36<00:00, 19.52it/s, v_num=2]Epoch  23 | Train Loss: 1.4252 | Train Acc: 0.4880\n",
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 30.21it/s, v_num=2]         | Val Loss:   1.3990 | Val Acc:   0.4954\n",
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 19.91it/s, v_num=2]Epoch  24 | Train Loss: 1.4202 | Train Acc: 0.4890\n",
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.74it/s, v_num=2]         | Val Loss:   1.4195 | Val Acc:   0.4852\n",
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 20.84it/s, v_num=2]Epoch  25 | Train Loss: 1.4125 | Train Acc: 0.4938\n",
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.00it/s, v_num=2]         | Val Loss:   1.4258 | Val Acc:   0.4832\n",
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.58it/s, v_num=2]Epoch  26 | Train Loss: 1.4073 | Train Acc: 0.4925\n",
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 32.04it/s, v_num=2]         | Val Loss:   1.4078 | Val Acc:   0.4900\n",
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.66it/s, v_num=2]Epoch  27 | Train Loss: 1.4024 | Train Acc: 0.4939\n",
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.18it/s, v_num=2]         | Val Loss:   1.4103 | Val Acc:   0.4900\n",
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.28it/s, v_num=2]Epoch  28 | Train Loss: 1.3931 | Train Acc: 0.4992\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.90it/s, v_num=2]         | Val Loss:   1.4062 | Val Acc:   0.4908\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.55it/s, v_num=2]Epoch  29 | Train Loss: 1.3870 | Train Acc: 0.5024\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.54it/s, v_num=2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 98.92it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_acc            0.5008999705314636\n",
      "        test_loss           1.4100968837738037\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.4100968837738037, 'test_acc': 0.5008999705314636}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.leaky_relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcda2238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Load the CSV file\\ncsv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\\ndf = pd.read_csv(csv_path)\\n\\n# Extract test metrics (they appear only once at the end)\\ntest_row = df[df[\\'test_loss\\'].notna()]\\nif not test_row.empty:\\n    test_loss = test_row[\\'test_loss\\'].values[0]\\n    test_acc = test_row[\\'test_acc\\'].values[0]\\n\\n# Create the plots\\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\\n# Loss plot\\naxes[0].plot(df[\\'epoch\\'], df[\\'train_loss_epoch\\'], label=\\'Train Loss\\', marker=\\'o\\')\\naxes[0].plot(df[\\'epoch\\'], df[\\'val_loss\\'], label=\\'Val Loss\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[0].axhline(y=test_loss, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Loss\\')\\naxes[0].set_xlabel(\\'Epoch\\')\\naxes[0].set_ylabel(\\'Loss\\')\\naxes[0].set_title(\\'Loss Over Epochs\\')\\naxes[0].legend()\\naxes[0].grid(True)\\n\\n# Accuracy plot\\naxes[1].plot(df[\\'epoch\\'], df[\\'train_acc_epoch\\'], label=\\'Train Acc\\', marker=\\'o\\')\\naxes[1].plot(df[\\'epoch\\'], df[\\'val_acc\\'], label=\\'Val Acc\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[1].axhline(y=test_acc, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Acc\\')\\naxes[1].set_xlabel(\\'Epoch\\')\\naxes[1].set_ylabel(\\'Accuracy\\')\\naxes[1].set_title(\\'Accuracy Over Epochs\\')\\naxes[1].legend()\\naxes[1].grid(True)\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Print test results\\nif not test_row.empty:\\n    print(f\"\\nFinal Test Results:\")\\n    print(f\"Test Loss: {test_loss:.4f}\")\\n    print(f\"Test Accuracy: {test_acc:.4f}\")\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ad718",
   "metadata": {},
   "source": [
    "As we can see, changing the activation function to a leaky_relu, improved the accuracy, the loss and allowed the network to learn for a few more epochs. we will try with a different activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5966f32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | hidden_layers | ModuleList | 403 K  | train\n",
      "1 | output_layer  | Linear     | 330    | train\n",
      "-----------------------------------------------------\n",
      "404 K     Trainable params\n",
      "0         Non-trainable params\n",
      "404 K     Total params\n",
      "1.616     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 45.47it/s]         | Val Loss:   2.3104 | Val Acc:   0.0938\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:12<00:00, 57.29it/s, v_num=3]                 | Val Loss:   1.9363 | Val Acc:   0.3038\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 29.16it/s, v_num=3]Epoch   0 | Train Loss: 2.0504 | Train Acc: 0.2595\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.91it/s, v_num=3]         | Val Loss:   1.8473 | Val Acc:   0.3328\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.37it/s, v_num=3]Epoch   1 | Train Loss: 1.8985 | Train Acc: 0.3185\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 29.50it/s, v_num=3]         | Val Loss:   1.7837 | Val Acc:   0.3492\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 19.68it/s, v_num=3]Epoch   2 | Train Loss: 1.8177 | Train Acc: 0.3472\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.47it/s, v_num=3]         | Val Loss:   1.7161 | Val Acc:   0.3818\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.70it/s, v_num=3]Epoch   3 | Train Loss: 1.7428 | Train Acc: 0.3742\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.34it/s, v_num=3]         | Val Loss:   1.6692 | Val Acc:   0.3934\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 20.75it/s, v_num=3]Epoch   4 | Train Loss: 1.6953 | Train Acc: 0.3892\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 32.47it/s, v_num=3]         | Val Loss:   1.6399 | Val Acc:   0.4086\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:33<00:00, 21.00it/s, v_num=3]Epoch   5 | Train Loss: 1.6559 | Train Acc: 0.4047\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 31.54it/s, v_num=3]         | Val Loss:   1.6148 | Val Acc:   0.4166\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:34<00:00, 20.22it/s, v_num=3]Epoch   6 | Train Loss: 1.6280 | Train Acc: 0.4138\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 30.70it/s, v_num=3]         | Val Loss:   1.6151 | Val Acc:   0.4200\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 20.01it/s, v_num=3]Epoch   7 | Train Loss: 1.6043 | Train Acc: 0.4216\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 30.84it/s, v_num=3]         | Val Loss:   1.5868 | Val Acc:   0.4242\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 19.93it/s, v_num=3]Epoch   8 | Train Loss: 1.5822 | Train Acc: 0.4306\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 30.31it/s, v_num=3]         | Val Loss:   1.5691 | Val Acc:   0.4324\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 20.05it/s, v_num=3]Epoch   9 | Train Loss: 1.5627 | Train Acc: 0.4391\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 30.05it/s, v_num=3]         | Val Loss:   1.5427 | Val Acc:   0.4392\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 19.59it/s, v_num=3]Epoch  10 | Train Loss: 1.5476 | Train Acc: 0.4423\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 30.27it/s, v_num=3]         | Val Loss:   1.5356 | Val Acc:   0.4474\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 19.96it/s, v_num=3]Epoch  11 | Train Loss: 1.5311 | Train Acc: 0.4470\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 30.69it/s, v_num=3]         | Val Loss:   1.5226 | Val Acc:   0.4458\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 20.00it/s, v_num=3]Epoch  12 | Train Loss: 1.5185 | Train Acc: 0.4531\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 29.51it/s, v_num=3]         | Val Loss:   1.5099 | Val Acc:   0.4564\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:36<00:00, 19.51it/s, v_num=3]Epoch  13 | Train Loss: 1.5004 | Train Acc: 0.4597\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 30.34it/s, v_num=3]         | Val Loss:   1.4946 | Val Acc:   0.4626\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 19.89it/s, v_num=3]Epoch  14 | Train Loss: 1.4937 | Train Acc: 0.4628\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 29.08it/s, v_num=3]         | Val Loss:   1.4790 | Val Acc:   0.4644\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:36<00:00, 19.37it/s, v_num=3]Epoch  15 | Train Loss: 1.4825 | Train Acc: 0.4681\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:22<00:00, 30.89it/s, v_num=3]         | Val Loss:   1.4750 | Val Acc:   0.4696\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 19.67it/s, v_num=3]Epoch  16 | Train Loss: 1.4721 | Train Acc: 0.4725\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:21<00:00, 32.12it/s, v_num=3]         | Val Loss:   1.4629 | Val Acc:   0.4760\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:32<00:00, 21.69it/s, v_num=3]Epoch  17 | Train Loss: 1.4591 | Train Acc: 0.4738\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:18<00:00, 37.89it/s, v_num=3]         | Val Loss:   1.4796 | Val Acc:   0.4572\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:30<00:00, 23.42it/s, v_num=3]Epoch  18 | Train Loss: 1.4531 | Train Acc: 0.4781\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:23<00:00, 30.06it/s, v_num=3]         | Val Loss:   1.4611 | Val Acc:   0.4690\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:35<00:00, 19.67it/s, v_num=3]Epoch  19 | Train Loss: 1.4445 | Train Acc: 0.4804\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 39.64it/s, v_num=3]         | Val Loss:   1.4484 | Val Acc:   0.4770\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:28<00:00, 25.13it/s, v_num=3]Epoch  20 | Train Loss: 1.4375 | Train Acc: 0.4832\n",
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.11it/s, v_num=3]         | Val Loss:   1.4503 | Val Acc:   0.4744\n",
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.11it/s, v_num=3]Epoch  21 | Train Loss: 1.4299 | Train Acc: 0.4851\n",
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.85it/s, v_num=3]         | Val Loss:   1.4347 | Val Acc:   0.4894\n",
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.71it/s, v_num=3]Epoch  22 | Train Loss: 1.4227 | Train Acc: 0.4872\n",
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.85it/s, v_num=3]         | Val Loss:   1.4226 | Val Acc:   0.4874\n",
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.36it/s, v_num=3]Epoch  23 | Train Loss: 1.4156 | Train Acc: 0.4902\n",
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.85it/s, v_num=3]         | Val Loss:   1.4407 | Val Acc:   0.4866\n",
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.63it/s, v_num=3]Epoch  24 | Train Loss: 1.4100 | Train Acc: 0.4912\n",
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.51it/s, v_num=3]         | Val Loss:   1.4161 | Val Acc:   0.4932\n",
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.25it/s, v_num=3]Epoch  25 | Train Loss: 1.4060 | Train Acc: 0.4925\n",
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.37it/s, v_num=3]         | Val Loss:   1.4148 | Val Acc:   0.4898\n",
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.96it/s, v_num=3]Epoch  26 | Train Loss: 1.3992 | Train Acc: 0.4961\n",
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.43it/s, v_num=3]         | Val Loss:   1.4202 | Val Acc:   0.4888\n",
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.22it/s, v_num=3]Epoch  27 | Train Loss: 1.3904 | Train Acc: 0.4994\n",
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.63it/s, v_num=3]         | Val Loss:   1.4015 | Val Acc:   0.4988\n",
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.62it/s, v_num=3]Epoch  28 | Train Loss: 1.3891 | Train Acc: 0.5022\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.27it/s, v_num=3]         | Val Loss:   1.3997 | Val Acc:   0.4960\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.73it/s, v_num=3]Epoch  29 | Train Loss: 1.3819 | Train Acc: 0.5042\n",
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.96it/s, v_num=3]         | Val Loss:   1.4004 | Val Acc:   0.4976\n",
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.64it/s, v_num=3]Epoch  30 | Train Loss: 1.3807 | Train Acc: 0.5029\n",
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.02it/s, v_num=3]         | Val Loss:   1.3893 | Val Acc:   0.5100\n",
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.99it/s, v_num=3]Epoch  31 | Train Loss: 1.3713 | Train Acc: 0.5056\n",
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.64it/s, v_num=3]         | Val Loss:   1.3742 | Val Acc:   0.4958\n",
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.76it/s, v_num=3]Epoch  32 | Train Loss: 1.3707 | Train Acc: 0.5076\n",
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.71it/s, v_num=3]         | Val Loss:   1.3833 | Val Acc:   0.5000\n",
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.90it/s, v_num=3]Epoch  33 | Train Loss: 1.3643 | Train Acc: 0.5105\n",
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.12it/s, v_num=3]         | Val Loss:   1.3745 | Val Acc:   0.5028\n",
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.96it/s, v_num=3]Epoch  34 | Train Loss: 1.3604 | Train Acc: 0.5097\n",
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.17it/s, v_num=3]         | Val Loss:   1.3808 | Val Acc:   0.4962\n",
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.95it/s, v_num=3]Epoch  35 | Train Loss: 1.3577 | Train Acc: 0.5129\n",
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.43it/s, v_num=3]         | Val Loss:   1.3761 | Val Acc:   0.5000\n",
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.69it/s, v_num=3]Epoch  36 | Train Loss: 1.3500 | Train Acc: 0.5136\n",
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.50it/s, v_num=3]         | Val Loss:   1.3754 | Val Acc:   0.4982\n",
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.92it/s, v_num=3]Epoch  37 | Train Loss: 1.3480 | Train Acc: 0.5174\n",
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.90it/s, v_num=3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 139.72it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_acc            0.5180000066757202\n",
      "        test_loss           1.3671553134918213\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.3671553134918213, 'test_acc': 0.5180000066757202}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.elu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b7738e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Load the CSV file\\ncsv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\\ndf = pd.read_csv(csv_path)\\n\\n# Extract test metrics (they appear only once at the end)\\ntest_row = df[df[\\'test_loss\\'].notna()]\\nif not test_row.empty:\\n    test_loss = test_row[\\'test_loss\\'].values[0]\\n    test_acc = test_row[\\'test_acc\\'].values[0]\\n\\n# Create the plots\\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\\n# Loss plot\\naxes[0].plot(df[\\'epoch\\'], df[\\'train_loss_epoch\\'], label=\\'Train Loss\\', marker=\\'o\\')\\naxes[0].plot(df[\\'epoch\\'], df[\\'val_loss\\'], label=\\'Val Loss\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[0].axhline(y=test_loss, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Loss\\')\\naxes[0].set_xlabel(\\'Epoch\\')\\naxes[0].set_ylabel(\\'Loss\\')\\naxes[0].set_title(\\'Loss Over Epochs\\')\\naxes[0].legend()\\naxes[0].grid(True)\\n\\n# Accuracy plot\\naxes[1].plot(df[\\'epoch\\'], df[\\'train_acc_epoch\\'], label=\\'Train Acc\\', marker=\\'o\\')\\naxes[1].plot(df[\\'epoch\\'], df[\\'val_acc\\'], label=\\'Val Acc\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[1].axhline(y=test_acc, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Acc\\')\\naxes[1].set_xlabel(\\'Epoch\\')\\naxes[1].set_ylabel(\\'Accuracy\\')\\naxes[1].set_title(\\'Accuracy Over Epochs\\')\\naxes[1].legend()\\naxes[1].grid(True)\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Print test results\\nif not test_row.empty:\\n    print(f\"\\nFinal Test Results:\")\\n    print(f\"Test Loss: {test_loss:.4f}\")\\n    print(f\"Test Accuracy: {test_acc:.4f}\")\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18538531",
   "metadata": {},
   "source": [
    "so far, this hasn't been better than the leaky relu, we will try with a different one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2f3ba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | hidden_layers | ModuleList | 403 K  | train\n",
      "1 | output_layer  | Linear     | 330    | train\n",
      "-----------------------------------------------------\n",
      "404 K     Trainable params\n",
      "0         Non-trainable params\n",
      "404 K     Total params\n",
      "1.616     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 89.03it/s]         | Val Loss:   2.2943 | Val Acc:   0.1250\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:08<00:00, 84.03it/s, v_num=4]                 | Val Loss:   1.9967 | Val Acc:   0.2834\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 39.58it/s, v_num=4]Epoch   0 | Train Loss: 2.0890 | Train Acc: 0.2461\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.19it/s, v_num=4]         | Val Loss:   1.9168 | Val Acc:   0.3236\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.08it/s, v_num=4]Epoch   1 | Train Loss: 1.9516 | Train Acc: 0.3010\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.29it/s, v_num=4]         | Val Loss:   1.8491 | Val Acc:   0.3456\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.12it/s, v_num=4]Epoch   2 | Train Loss: 1.8799 | Train Acc: 0.3298\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 41.26it/s, v_num=4]         | Val Loss:   1.7960 | Val Acc:   0.3568\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:27<00:00, 26.02it/s, v_num=4]Epoch   3 | Train Loss: 1.8200 | Train Acc: 0.3498\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 41.32it/s, v_num=4]         | Val Loss:   1.7460 | Val Acc:   0.3698\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.52it/s, v_num=4]Epoch   4 | Train Loss: 1.7660 | Train Acc: 0.3666\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.82it/s, v_num=4]         | Val Loss:   1.7154 | Val Acc:   0.3794\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.80it/s, v_num=4]Epoch   5 | Train Loss: 1.7242 | Train Acc: 0.3809\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.19it/s, v_num=4]         | Val Loss:   1.6823 | Val Acc:   0.3984\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.79it/s, v_num=4]Epoch   6 | Train Loss: 1.6934 | Train Acc: 0.3919\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.86it/s, v_num=4]         | Val Loss:   1.6610 | Val Acc:   0.4012\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.65it/s, v_num=4]Epoch   7 | Train Loss: 1.6630 | Train Acc: 0.3994\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.45it/s, v_num=4]         | Val Loss:   1.6470 | Val Acc:   0.4044\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.80it/s, v_num=4]Epoch   8 | Train Loss: 1.6397 | Train Acc: 0.4100\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.50it/s, v_num=4]         | Val Loss:   1.6074 | Val Acc:   0.4234\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.96it/s, v_num=4]Epoch   9 | Train Loss: 1.6163 | Train Acc: 0.4211\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.95it/s, v_num=4]         | Val Loss:   1.5897 | Val Acc:   0.4220\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.70it/s, v_num=4]Epoch  10 | Train Loss: 1.6005 | Train Acc: 0.4254\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.02it/s, v_num=4]         | Val Loss:   1.5763 | Val Acc:   0.4266\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.76it/s, v_num=4]Epoch  11 | Train Loss: 1.5796 | Train Acc: 0.4317\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.58it/s, v_num=4]         | Val Loss:   1.5746 | Val Acc:   0.4364\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.21it/s, v_num=4]Epoch  12 | Train Loss: 1.5646 | Train Acc: 0.4375\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.76it/s, v_num=4]         | Val Loss:   1.5832 | Val Acc:   0.4204\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.83it/s, v_num=4]Epoch  13 | Train Loss: 1.5588 | Train Acc: 0.4398\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:17<00:00, 39.87it/s, v_num=4]         | Val Loss:   1.5627 | Val Acc:   0.4362\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.10it/s, v_num=4]Epoch  14 | Train Loss: 1.5455 | Train Acc: 0.4459\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.14it/s, v_num=4]         | Val Loss:   1.5443 | Val Acc:   0.4542\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.13it/s, v_num=4]Epoch  15 | Train Loss: 1.5330 | Train Acc: 0.4473\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.06it/s, v_num=4]         | Val Loss:   1.5469 | Val Acc:   0.4436\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.10it/s, v_num=4]Epoch  16 | Train Loss: 1.5273 | Train Acc: 0.4505\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.59it/s, v_num=4]         | Val Loss:   1.5265 | Val Acc:   0.4474\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.34it/s, v_num=4]Epoch  17 | Train Loss: 1.5173 | Train Acc: 0.4530\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.64it/s, v_num=4]         | Val Loss:   1.5381 | Val Acc:   0.4456\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.34it/s, v_num=4]Epoch  18 | Train Loss: 1.5115 | Train Acc: 0.4608\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.36it/s, v_num=4]         | Val Loss:   1.5119 | Val Acc:   0.4494\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.16it/s, v_num=4]Epoch  19 | Train Loss: 1.5007 | Train Acc: 0.4590\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.50it/s, v_num=4]         | Val Loss:   1.5196 | Val Acc:   0.4556\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.04it/s, v_num=4]Epoch  20 | Train Loss: 1.4947 | Train Acc: 0.4606\n",
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.72it/s, v_num=4]         | Val Loss:   1.5131 | Val Acc:   0.4466\n",
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.26it/s, v_num=4]Epoch  21 | Train Loss: 1.4892 | Train Acc: 0.4664\n",
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.45it/s, v_num=4]         | Val Loss:   1.4933 | Val Acc:   0.4614\n",
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.15it/s, v_num=4]Epoch  22 | Train Loss: 1.4813 | Train Acc: 0.4676\n",
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.95it/s, v_num=4]         | Val Loss:   1.4913 | Val Acc:   0.4664\n",
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.33it/s, v_num=4]Epoch  23 | Train Loss: 1.4827 | Train Acc: 0.4682\n",
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.44it/s, v_num=4]         | Val Loss:   1.4952 | Val Acc:   0.4698\n",
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.19it/s, v_num=4]Epoch  24 | Train Loss: 1.4755 | Train Acc: 0.4718\n",
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 41.77it/s, v_num=4]         | Val Loss:   1.4986 | Val Acc:   0.4600\n",
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.01it/s, v_num=4]Epoch  25 | Train Loss: 1.4697 | Train Acc: 0.4745\n",
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.29it/s, v_num=4]         | Val Loss:   1.4954 | Val Acc:   0.4614\n",
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.82it/s, v_num=4]Epoch  26 | Train Loss: 1.4644 | Train Acc: 0.4737\n",
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.38it/s, v_num=4]         | Val Loss:   1.5003 | Val Acc:   0.4576\n",
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.53it/s, v_num=4]Epoch  27 | Train Loss: 1.4637 | Train Acc: 0.4739\n",
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.46it/s, v_num=4]         | Val Loss:   1.4843 | Val Acc:   0.4676\n",
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.83it/s, v_num=4]Epoch  28 | Train Loss: 1.4616 | Train Acc: 0.4725\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.95it/s, v_num=4]         | Val Loss:   1.4814 | Val Acc:   0.4668\n",
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.65it/s, v_num=4]Epoch  29 | Train Loss: 1.4564 | Train Acc: 0.4790\n",
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.57it/s, v_num=4]         | Val Loss:   1.4839 | Val Acc:   0.4680\n",
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.88it/s, v_num=4]Epoch  30 | Train Loss: 1.4514 | Train Acc: 0.4798\n",
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.21it/s, v_num=4]         | Val Loss:   1.4798 | Val Acc:   0.4658\n",
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.68it/s, v_num=4]Epoch  31 | Train Loss: 1.4484 | Train Acc: 0.4810\n",
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.49it/s, v_num=4]         | Val Loss:   1.4932 | Val Acc:   0.4650\n",
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.80it/s, v_num=4]Epoch  32 | Train Loss: 1.4473 | Train Acc: 0.4816\n",
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.16it/s, v_num=4]         | Val Loss:   1.4692 | Val Acc:   0.4604\n",
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.67it/s, v_num=4]Epoch  33 | Train Loss: 1.4408 | Train Acc: 0.4835\n",
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.51it/s, v_num=4]         | Val Loss:   1.4694 | Val Acc:   0.4736\n",
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.72it/s, v_num=4]Epoch  34 | Train Loss: 1.4341 | Train Acc: 0.4862\n",
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.02it/s, v_num=4]         | Val Loss:   1.4765 | Val Acc:   0.4666\n",
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.42it/s, v_num=4]Epoch  35 | Train Loss: 1.4368 | Train Acc: 0.4846\n",
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.33it/s, v_num=4]         | Val Loss:   1.4690 | Val Acc:   0.4708\n",
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.72it/s, v_num=4]Epoch  36 | Train Loss: 1.4336 | Train Acc: 0.4855\n",
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.00it/s, v_num=4]         | Val Loss:   1.4702 | Val Acc:   0.4748\n",
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.55it/s, v_num=4]Epoch  37 | Train Loss: 1.4310 | Train Acc: 0.4866\n",
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.44it/s, v_num=4]         | Val Loss:   1.4595 | Val Acc:   0.4722\n",
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.59it/s, v_num=4]Epoch  38 | Train Loss: 1.4293 | Train Acc: 0.4863\n",
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.16it/s, v_num=4]         | Val Loss:   1.4601 | Val Acc:   0.4650\n",
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.60it/s, v_num=4]Epoch  39 | Train Loss: 1.4261 | Train Acc: 0.4882\n",
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.19it/s, v_num=4]         | Val Loss:   1.4596 | Val Acc:   0.4748\n",
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.14it/s, v_num=4]Epoch  40 | Train Loss: 1.4166 | Train Acc: 0.4917\n",
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.19it/s, v_num=4]         | Val Loss:   1.4589 | Val Acc:   0.4688\n",
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.48it/s, v_num=4]Epoch  41 | Train Loss: 1.4230 | Train Acc: 0.4893\n",
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.03it/s, v_num=4]         | Val Loss:   1.4364 | Val Acc:   0.4808\n",
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.11it/s, v_num=4]Epoch  42 | Train Loss: 1.4168 | Train Acc: 0.4887\n",
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.85it/s, v_num=4]         | Val Loss:   1.4426 | Val Acc:   0.4822\n",
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.56it/s, v_num=4]Epoch  43 | Train Loss: 1.4089 | Train Acc: 0.4943\n",
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.61it/s, v_num=4]         | Val Loss:   1.4505 | Val Acc:   0.4786\n",
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.29it/s, v_num=4]Epoch  44 | Train Loss: 1.4115 | Train Acc: 0.4917\n",
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 43.20it/s, v_num=4]         | Val Loss:   1.4358 | Val Acc:   0.4844\n",
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.57it/s, v_num=4]Epoch  45 | Train Loss: 1.4123 | Train Acc: 0.4936\n",
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:16<00:00, 42.64it/s, v_num=4]         | Val Loss:   1.4520 | Val Acc:   0.4816\n",
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:31<00:00, 22.39it/s, v_num=4]Epoch  46 | Train Loss: 1.4062 | Train Acc: 0.4945\n",
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.72it/s, v_num=4]         | Val Loss:   1.4495 | Val Acc:   0.4780\n",
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:36<00:00, 19.31it/s, v_num=4]Epoch  47 | Train Loss: 1.4062 | Train Acc: 0.4956\n",
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.20it/s, v_num=4]         | Val Loss:   1.4351 | Val Acc:   0.4790\n",
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 18.80it/s, v_num=4]Epoch  48 | Train Loss: 1.4012 | Train Acc: 0.5007\n",
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:27<00:00, 25.53it/s, v_num=4]         | Val Loss:   1.4470 | Val Acc:   0.4728\n",
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:40<00:00, 17.31it/s, v_num=4]Epoch  49 | Train Loss: 1.4008 | Train Acc: 0.4956\n",
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.46it/s, v_num=4]         | Val Loss:   1.4419 | Val Acc:   0.4812\n",
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 18.66it/s, v_num=4]Epoch  50 | Train Loss: 1.3956 | Train Acc: 0.4977\n",
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 28.11it/s, v_num=4]         | Val Loss:   1.4328 | Val Acc:   0.4806\n",
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:37<00:00, 18.65it/s, v_num=4]Epoch  51 | Train Loss: 1.3972 | Train Acc: 0.5008\n",
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.59it/s, v_num=4]         | Val Loss:   1.4309 | Val Acc:   0.4826\n",
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 18.04it/s, v_num=4]Epoch  52 | Train Loss: 1.3942 | Train Acc: 0.4978\n",
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.58it/s, v_num=4]         | Val Loss:   1.4355 | Val Acc:   0.4812\n",
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:42<00:00, 16.43it/s, v_num=4]Epoch  53 | Train Loss: 1.3914 | Train Acc: 0.5022\n",
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.69it/s, v_num=4]         | Val Loss:   1.4303 | Val Acc:   0.4840\n",
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.97it/s, v_num=4]Epoch  54 | Train Loss: 1.3902 | Train Acc: 0.5019\n",
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.41it/s, v_num=4]         | Val Loss:   1.4486 | Val Acc:   0.4752\n",
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.29it/s, v_num=4]Epoch  55 | Train Loss: 1.3835 | Train Acc: 0.5062\n",
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.51it/s, v_num=4]         | Val Loss:   1.4392 | Val Acc:   0.4788\n",
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.23it/s, v_num=4]Epoch  56 | Train Loss: 1.3841 | Train Acc: 0.5046\n",
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.04it/s, v_num=4]         | Val Loss:   1.4306 | Val Acc:   0.4836\n",
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.98it/s, v_num=4]Epoch  57 | Train Loss: 1.3810 | Train Acc: 0.5036\n",
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.54it/s, v_num=4]         | Val Loss:   1.4347 | Val Acc:   0.4800\n",
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.87it/s, v_num=4]Epoch  58 | Train Loss: 1.3835 | Train Acc: 0.5041\n",
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.33it/s, v_num=4]         | Val Loss:   1.4239 | Val Acc:   0.4818\n",
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.26it/s, v_num=4]Epoch  59 | Train Loss: 1.3813 | Train Acc: 0.5045\n",
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.51it/s, v_num=4]         | Val Loss:   1.4141 | Val Acc:   0.4864\n",
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:40<00:00, 17.54it/s, v_num=4]Epoch  60 | Train Loss: 1.3785 | Train Acc: 0.5047\n",
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:27<00:00, 25.62it/s, v_num=4]         | Val Loss:   1.4163 | Val Acc:   0.4886\n",
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:40<00:00, 17.25it/s, v_num=4]Epoch  61 | Train Loss: 1.3763 | Train Acc: 0.5073\n",
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.47it/s, v_num=4]         | Val Loss:   1.4250 | Val Acc:   0.4864\n",
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:40<00:00, 17.43it/s, v_num=4]Epoch  62 | Train Loss: 1.3795 | Train Acc: 0.5066\n",
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.00it/s, v_num=4]         | Val Loss:   1.4081 | Val Acc:   0.4982\n",
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.08it/s, v_num=4]Epoch  63 | Train Loss: 1.3762 | Train Acc: 0.5070\n",
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:27<00:00, 25.59it/s, v_num=4]         | Val Loss:   1.4132 | Val Acc:   0.4970\n",
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:40<00:00, 17.46it/s, v_num=4]Epoch  64 | Train Loss: 1.3731 | Train Acc: 0.5085\n",
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:25<00:00, 27.44it/s, v_num=4]         | Val Loss:   1.4176 | Val Acc:   0.4972\n",
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.20it/s, v_num=4]Epoch  65 | Train Loss: 1.3751 | Train Acc: 0.5076\n",
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:24<00:00, 28.21it/s, v_num=4]         | Val Loss:   1.4167 | Val Acc:   0.4946\n",
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:38<00:00, 18.46it/s, v_num=4]Epoch  66 | Train Loss: 1.3682 | Train Acc: 0.5101\n",
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 27.01it/s, v_num=4]         | Val Loss:   1.4211 | Val Acc:   0.4838\n",
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:39<00:00, 17.99it/s, v_num=4]Epoch  67 | Train Loss: 1.3663 | Train Acc: 0.5096\n",
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:26<00:00, 26.42it/s, v_num=4]         | Val Loss:   1.4092 | Val Acc:   0.4988\n",
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:40<00:00, 17.18it/s, v_num=4]Epoch  68 | Train Loss: 1.3678 | Train Acc: 0.5112\n",
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [00:41<00:00, 17.17it/s, v_num=4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 97.61it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_acc            0.3889999985694885\n",
      "        test_loss           1.8465255498886108\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.8465255498886108, 'test_acc': 0.3889999985694885}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.tanh  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a42888e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Load the CSV file\\ncsv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\\ndf = pd.read_csv(csv_path)\\n\\n# Extract test metrics (they appear only once at the end)\\ntest_row = df[df[\\'test_loss\\'].notna()]\\nif not test_row.empty:\\n    test_loss = test_row[\\'test_loss\\'].values[0]\\n    test_acc = test_row[\\'test_acc\\'].values[0]\\n\\n# Create the plots\\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\\n# Loss plot\\naxes[0].plot(df[\\'epoch\\'], df[\\'train_loss_epoch\\'], label=\\'Train Loss\\', marker=\\'o\\')\\naxes[0].plot(df[\\'epoch\\'], df[\\'val_loss\\'], label=\\'Val Loss\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[0].axhline(y=test_loss, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Loss\\')\\naxes[0].set_xlabel(\\'Epoch\\')\\naxes[0].set_ylabel(\\'Loss\\')\\naxes[0].set_title(\\'Loss Over Epochs\\')\\naxes[0].legend()\\naxes[0].grid(True)\\n\\n# Accuracy plot\\naxes[1].plot(df[\\'epoch\\'], df[\\'train_acc_epoch\\'], label=\\'Train Acc\\', marker=\\'o\\')\\naxes[1].plot(df[\\'epoch\\'], df[\\'val_acc\\'], label=\\'Val Acc\\', marker=\\'s\\')\\nif not test_row.empty:\\n    axes[1].axhline(y=test_acc, color=\\'r\\', linestyle=\\'--\\', label=\\'Test Acc\\')\\naxes[1].set_xlabel(\\'Epoch\\')\\naxes[1].set_ylabel(\\'Accuracy\\')\\naxes[1].set_title(\\'Accuracy Over Epochs\\')\\naxes[1].legend()\\naxes[1].grid(True)\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Print test results\\nif not test_row.empty:\\n    print(f\"\\nFinal Test Results:\")\\n    print(f\"Test Loss: {test_loss:.4f}\")\\n    print(f\"Test Accuracy: {test_acc:.4f}\")\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53eb48",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.functional' has no attribute 'Sigmoid'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m EPOCHS = \u001b[32m100\u001b[39m\n\u001b[32m      7\u001b[39m HIDDEN_SIZES = [ \u001b[32m128\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m32\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m ACTIVATION_FN = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSigmoid\u001b[49m  \u001b[38;5;66;03m# or F.leaky_relu, torch.sigmoid, torch.tanh\u001b[39;00m\n\u001b[32m      9\u001b[39m LEARNING_RATE = \u001b[32m0.001\u001b[39m\n\u001b[32m     10\u001b[39m OPTIMIZER = \u001b[33m'\u001b[39m\u001b[33msgd\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# 'sgd', 'adam', 'rmsprop'\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch.nn.functional' has no attribute 'Sigmoid'"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.sigmoid  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b3777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1c8a7014e50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 665.375x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7972f8",
   "metadata": {},
   "source": [
    "as we can see, we weren't able to find any activation function that was better than the leaky relu. Looking at the graph, we can clearly see that both the values of the accuracy and the loss are way worst than previous attempts. We can also see that it was getting better over the epochs but it reached the previously established epoch limit. Our theory is that, with enough epochs, it would reach around the same values as the other functions, and then starting to flatline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6cd886",
   "metadata": {},
   "source": [
    "# ///////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6abf8",
   "metadata": {},
   "source": [
    "# Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62611fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 1\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.5\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dab8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.1\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d11750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.05\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.05\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"logs/hs[128, 64, 32]_optsgd_lr0.001/version_0/metrics.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract test metrics (they appear only once at the end)\n",
    "test_row = df[df['test_loss'].notna()]\n",
    "if not test_row.empty:\n",
    "    test_loss = test_row['test_loss'].values[0]\n",
    "    test_acc = test_row['test_acc'].values[0]\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(df['epoch'], df['train_loss_epoch'], label='Train Loss', marker='o')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[0].axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(df['epoch'], df['train_acc_epoch'], label='Train Acc', marker='o')\n",
    "axes[1].plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n",
    "if not test_row.empty:\n",
    "    axes[1].axhline(y=test_acc, color='r', linestyle='--', label='Test Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "if not test_row.empty:\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f2877",
   "metadata": {},
   "source": [
    "# Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ac601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c153c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to test\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "HIDDEN_SIZES = [ 128, 64, 32]\n",
    "ACTIVATION_FN = F.relu  # or F.leaky_relu, torch.sigmoid, torch.tanh\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'sgd'  # 'sgd', 'adam', 'rmsprop'\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = False\n",
    "WEIGHT_INIT = None\n",
    "MOMENTUM = 0.9  # 'xavier', 'he', 'normal'\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create model\n",
    "model = LitANN(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer_name=OPTIMIZER,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    use_batch_norm=USE_BATCH_NORM,\n",
    "    weight_init=WEIGHT_INIT,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"logs\", \n",
    "    name=f\"hs{HIDDEN_SIZES}_opt{OPTIMIZER}_lr{LEARNING_RATE}\"\n",
    ")\n",
    "\n",
    "# Setup callbacks and logger\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "class EpochProgressCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch\n",
    "        train_loss = metrics.get('train_loss_epoch', metrics.get('train_loss', 0))\n",
    "        train_acc = metrics.get('train_acc_epoch', metrics.get('train_acc', 0))\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_acc = metrics.get('val_acc', 0)\n",
    "        print(f\"         | Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "# Train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, EpochProgressCallback()],\n",
    "    logger=csv_logger, enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
